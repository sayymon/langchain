{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ffUdoTetTjHOx3HIGi-0nvvg31e9cVYU","timestamp":1724761843583}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Projeto 03 - Converse com documentos usando RAG avanÃ§ada\n","\n","> Nesse projeto iremos aprender a como criar uma pipeline de RAG mais avanÃ§ada para que seja capaz de:\n","* fazer perguntas a algum documento lido, como se fosse um chat com o prÃ³prio arquivo.\n","* consultar mais de uma referÃªncia ao mesmo tempo.\n","* entender o contexto das mensagens passadas, usando o histÃ³rico da conversa tambÃ©m como uma referÃªncia para formular a resposta\n","\n","E para essa aplicaÃ§Ã£o tambÃ©m serÃ¡ construÃ­da uma interface.\n","Portanto, podemos reaproveitar parte do cÃ³digo do projeto anterior e assim ir adicionando as novas funcionalidades"],"metadata":{"id":"CInd-FXyVoZC"}},{"cell_type":"markdown","source":["## [ ! ] Como executar em ambiente local\n","Para executar o cÃ³digo desse projeto em um ambiente local, siga as instruÃ§Ãµes para instalar as dependÃªncias necessÃ¡rias usando os comandos abaixo. VocÃª pode usar os mesmos comandos de instalaÃ§Ã£o.\n","\n"],"metadata":{"id":"2Khq09Wegw2H"}},{"cell_type":"markdown","source":["## InstalaÃ§Ã£o e ConfiguraÃ§Ã£o\n","\n","Aqui iremos carregar primeiramente todos as funÃ§Ãµes que usamos no projeto anterior e mais algumas outras. Entra elas, o FAISS (um vectorstore no mesmo estilo do Chroma, que usamos nas aulas anteriores sobre RAG) e tambÃ©m outras funÃ§Ãµes necessÃ¡rias para implementaÃ§Ã£o de uma pipeline RAG que entenda o contexto das conversas.\n","\n","Lembrando: podemos reaproveitar parte do cÃ³digo que criamos no projeto 02.\n","EntÃ£o se quiser pode fazer uma cÃ³pia e fazer as modificaÃ§Ãµes a partir dele.\n","\n","Abaixo, cada mudanÃ§a que serÃ¡ feita a partir desse arquivo, assim poderÃ¡ ir acompanhando as alteraÃ§Ãµes"],"metadata":{"id":"EqT4YzGtWZGq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEG8KS746Wmu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725479464760,"user_tz":180,"elapsed":27249,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"69f90cc6-5a7b-4722-c2f4-11def4662225"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q streamlit langchain\n","!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"]},{"cell_type":"markdown","source":["### InstalaÃ§Ã£o do FAISS\n","\n","Antes de importar Ã© necessÃ¡rio que instalemos o FAISS, ele nÃ£o vem instalado por padrÃ£o no Colab. Portanto, podemos usar aqui e em ambiente local esse mesmo comando para instalar:\n","\n","`pip install -q faiss-cpu`\n","\n","vocÃª tambÃ©m pode instalar `faiss-gpu` caso queira usar a versÃ£o otimizada para GPU. Para simplificar no momento, iremos usar a versÃ£o padrÃ£o por CPU mesmo\n","\n"],"metadata":{"id":"tv4u-p4J0l9K"}},{"cell_type":"code","source":["!pip install -q faiss-cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j-hJoWdJEBmU","executionInfo":{"status":"ok","timestamp":1725479492860,"user_tz":180,"elapsed":7497,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"c084202a-7be4-4bbc-dbd4-5ced127fef74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["Em seguida, use em sua aplicaÃ§Ã£o o `import faiss` e tambÃ©m importar o `FAISS` dentro de biblioteca langchain"],"metadata":{"id":"paEkaqQI0o7V"}},{"cell_type":"code","source":["import faiss\n","from langchain_community.vectorstores import FAISS"],"metadata":{"id":"MP19iHfeEETw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### InstalaÃ§Ã£o do PyPDFLoader\n","\n","Usaremos o PyPDFLoader para fazer a leitura dos arquivos PDF em nossa aplicaÃ§Ã£o. Isso serÃ¡ explicado com detalhes na devida seÃ§Ã£o.\n","E para podermos usÃ¡-lo, precisamos antes instalar a biblioteca com o comando abaixo"],"metadata":{"id":"ULIGaJSU0qca"}},{"cell_type":"code","source":["!pip install pypdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jf2iNG4hEGo-","executionInfo":{"status":"ok","timestamp":1725479508606,"user_tz":180,"elapsed":7319,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"db37a160-96f4-4d65-d633-68699063c8a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pypdf\n","  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n","Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n","\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/295.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m286.7/295.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","Successfully installed pypdf-4.3.1\n"]}]},{"cell_type":"markdown","source":["### Demais instalaÃ§Ãµes\n","\n","Assim como no projeto anterior, vamos instalar aqui o dotenv de novo (em ambiente local nÃ£o precisa executar a instalaÃ§Ã£o de novo, mas aqui no Colab como Ã© uma nova sessÃ£o precisamos) e tambÃ©m o localtunnel (lembrando que esse nÃ£o Ã© necessÃ¡rio em ambiente local)."],"metadata":{"id":"yUTV--pa0tHW"}},{"cell_type":"code","source":["!pip install -q python-dotenv\n","!npm install -q localtunnel"],"metadata":{"id":"xo38vjjlEJb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile .env\n","HUGGINGFACE_API_KEY=##########\n","HUGGINGFACEHUB_API_TOKEN=##########\n","OPENAI_API_KEY=##########\n","TAVILY_API_KEY=##########\n","SERPAPI_API_KEY=##########\n","LANGCHAIN_API_KEY=##########"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHxWcGDqEQZ0","executionInfo":{"status":"ok","timestamp":1725479528725,"user_tz":180,"elapsed":331,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"e6d4345f-d0c7-43bd-e886-958b47a83ffa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing .env\n"]}]},{"cell_type":"markdown","source":["---\n","\n","## InicializaÃ§Ã£o da interface\n","\n","Por fim, reunimos todo o cÃ³digo em um Ãºnico script e adicionamos a configuraÃ§Ã£o da pÃ¡gina com st.set_page_config e st.title, alterando o tÃ­tulo e o emoji para deixar a interface mais personalizada e adequada ao projeto atual, com um visual mais alinhado com o contexto desse projeto"],"metadata":{"id":"gmGl3IJyUB0U"}},{"cell_type":"code","source":["%%writefile projeto3.py\n","\n","import streamlit as st\n","from langchain_core.messages import AIMessage, HumanMessage\n","from langchain_core.prompts import MessagesPlaceholder\n","\n","from langchain_ollama import ChatOllama\n","from langchain_openai import ChatOpenAI\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n","\n","import torch\n","from langchain_huggingface import ChatHuggingFace\n","from langchain_community.llms import HuggingFaceHub\n","\n","import faiss\n","import tempfile\n","import os\n","import time\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","# ConfiguraÃ§Ãµes do Streamlit\n","st.set_page_config(page_title=\"Converse com documentos ğŸ“š\", page_icon=\"ğŸ“š\")\n","st.title(\"Converse com documentos ğŸ“š\")\n","\n","model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n","\n","## Provedores de modelos\n","def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n","  llm = HuggingFaceHub(\n","      repo_id=model,\n","      model_kwargs={\n","          \"temperature\": temperature,\n","          \"return_full_text\": False,\n","          \"max_new_tokens\": 512,\n","          #\"stop\": [\"<|eot_id|>\"],\n","          # demais parÃ¢metros que desejar\n","      }\n","  )\n","  return llm\n","\n","def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n","    llm = ChatOpenAI(\n","        model=model,\n","        temperature=temperature\n","        # demais parÃ¢metros que desejar\n","    )\n","    return llm\n","\n","def model_ollama(model=\"phi3\", temperature=0.1):\n","    llm = ChatOllama(\n","        model=model,\n","        temperature=temperature,\n","    )\n","    return llm\n","\n","\n","## IndexaÃ§Ã£o e RecuperaÃ§Ã£o\n","\n","def config_retriever(uploads):\n","    # Carregar documentos\n","    docs = []\n","    temp_dir = tempfile.TemporaryDirectory()\n","    for file in uploads:\n","        temp_filepath = os.path.join(temp_dir.name, file.name)\n","        with open(temp_filepath, \"wb\") as f:\n","            f.write(file.getvalue())\n","        loader = PyPDFLoader(temp_filepath)\n","        docs.extend(loader.load())\n","\n","    # DivisÃ£o em pedaÃ§os de texto / Split\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=1000,\n","        chunk_overlap=200\n","    )\n","    splits = text_splitter.split_documents(docs)\n","\n","    # Embeddings\n","    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n","\n","    # Armazenamento\n","    vectorstore = FAISS.from_documents(splits, embeddings)\n","\n","    vectorstore.save_local('vectorstore/db_faiss')\n","\n","    # Configurando o recuperador de texto / Retriever\n","    retriever = vectorstore.as_retriever(\n","        search_type='mmr',\n","        search_kwargs={'k':3, 'fetch_k':4}\n","    )\n","\n","    return retriever\n","\n","\n","def config_rag_chain(model_class, retriever):\n","\n","    ### Carregamento da LLM\n","    if model_class == \"hf_hub\":\n","        llm = model_hf_hub()\n","    elif model_class == \"openai\":\n","        llm = model_openai()\n","    elif model_class == \"ollama\":\n","        llm = model_ollama()\n","\n","    # Para definiÃ§Ã£o dos prompts\n","    if model_class.startswith(\"hf\"):\n","        token_s, token_e = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\", \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n","    else:\n","        token_s, token_e = \"\", \"\"\n","\n","    # Prompt de contextualizaÃ§Ã£o\n","    context_q_system_prompt = \"Given the following chat history and the follow-up question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n","\n","    context_q_system_prompt = token_s + context_q_system_prompt\n","    context_q_user_prompt = \"Question: {input}\" + token_e\n","    context_q_prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", context_q_system_prompt),\n","            MessagesPlaceholder(\"chat_history\"),\n","            (\"human\", context_q_user_prompt),\n","        ]\n","    )\n","\n","    # Chain para contextualizaÃ§Ã£o\n","    history_aware_retriever = create_history_aware_retriever(\n","        llm=llm, retriever=retriever, prompt=context_q_prompt\n","    )\n","\n","    # Prompt para perguntas e respostas (Q&A)\n","    qa_prompt_template = \"\"\"VocÃª Ã© um assistente virtual prestativo e estÃ¡ respondendo perguntas gerais.\n","    Use os seguintes pedaÃ§os de contexto recuperado para responder Ã  pergunta.\n","    Se vocÃª nÃ£o sabe a resposta, apenas diga que nÃ£o sabe. Mantenha a resposta concisa.\n","    Responda em portuguÃªs. \\n\\n\n","    Pergunta: {input} \\n\n","    Contexto: {context}\"\"\"\n","\n","    qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)\n","\n","    # Configurar LLM e Chain para perguntas e respostas (Q&A)\n","\n","    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n","\n","    rag_chain = create_retrieval_chain(\n","        history_aware_retriever,\n","        qa_chain,\n","    )\n","\n","    return rag_chain\n","\n","\n","## Cria painel lateral na interface\n","uploads = st.sidebar.file_uploader(\n","    label=\"Enviar arquivos\", type=[\"pdf\"],\n","    accept_multiple_files=True\n",")\n","if not uploads:\n","    st.info(\"Por favor, envie algum arquivo para continuar!\")\n","    st.stop()\n","\n","\n","if \"chat_history\" not in st.session_state:\n","    st.session_state.chat_history = [\n","        AIMessage(content=\"OlÃ¡, sou o seu assistente virtual! Como posso ajudar vocÃª?\"),\n","    ]\n","\n","if \"docs_list\" not in st.session_state:\n","    st.session_state.docs_list = None\n","\n","if \"retriever\" not in st.session_state:\n","    st.session_state.retriever = None\n","\n","for message in st.session_state.chat_history:\n","    if isinstance(message, AIMessage):\n","        with st.chat_message(\"AI\"):\n","            st.write(message.content)\n","    elif isinstance(message, HumanMessage):\n","        with st.chat_message(\"Human\"):\n","            st.write(message.content)\n","\n","# para gravar quanto tempo levou para a geraÃ§Ã£o\n","start = time.time()\n","user_query = st.chat_input(\"Digite sua mensagem aqui...\")\n","\n","if user_query is not None and user_query != \"\" and uploads is not None:\n","\n","    st.session_state.chat_history.append(HumanMessage(content=user_query))\n","\n","    with st.chat_message(\"Human\"):\n","        st.markdown(user_query)\n","\n","    with st.chat_message(\"AI\"):\n","\n","        if st.session_state.docs_list != uploads:\n","            print(uploads)\n","            st.session_state.docs_list = uploads\n","            st.session_state.retriever = config_retriever(uploads)\n","\n","        rag_chain = config_rag_chain(model_class, st.session_state.retriever)\n","\n","        result = rag_chain.invoke({\"input\": user_query, \"chat_history\": st.session_state.chat_history})\n","\n","        resp = result['answer']\n","        st.write(resp)\n","\n","        # mostrar a fonte\n","        sources = result['context']\n","        for idx, doc in enumerate(sources):\n","            source = doc.metadata['source']\n","            file = os.path.basename(source)\n","            page = doc.metadata.get('page', 'PÃ¡gina nÃ£o especificada')\n","\n","            ref = f\":link: Fonte {idx}: *{file} - p. {page}*\"\n","            print(ref)\n","            with st.popover(ref):\n","                st.caption(doc.page_content)\n","\n","    st.session_state.chat_history.append(AIMessage(content=resp))\n","\n","end = time.time()\n","print(\"Tempo: \", end - start)"],"metadata":{"id":"0P5CTavqvh74","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725479555039,"user_tz":180,"elapsed":405,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"a6af8c5e-2098-4917-bbe2-4ba9853380b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing projeto3.py\n"]}]},{"cell_type":"markdown","source":["### ExecuÃ§Ã£o do Streamlit\n","\n","\n"],"metadata":{"id":"M54-PNNEPrxh"}},{"cell_type":"code","source":["!streamlit run projeto3.py &>/content/logs.txt &"],"metadata":{"id":"KrnuMrbds4Na"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["E agora para nos conectar, usamos o comando abaixo (explicaÃ§Ãµes no colab do projeto 02)"],"metadata":{"id":"yYiheBeuN8SA"}},{"cell_type":"code","source":["!wget -q -O - ipv4.icanhazip.com\n","\n","!npx localtunnel --port 8501"],"metadata":{"id":"IMjgsW8c1NvZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725479806277,"user_tz":180,"elapsed":195555,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"356f748a-9487-4dd9-a7fe-f78f7b3928f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["34.125.205.79\n","your url is: https://salty-onions-melt.loca.lt\n","^C\n"]}]},{"cell_type":"markdown","source":["## Como melhorar ğŸš€\n","\n","Sabendo exatamente como cada funÃ§Ã£o opera e entendendo as explicaÃ§Ãµes tratadas ao longo deste projeto, vocÃª tem o conhecimento necessÃ¡rio para melhorar os resultados da sua aplicaÃ§Ã£o RAG. Vamos listar abaixo as estratÃ©gias que podem ser aplicadas para otimizar a qualidade das respostas e a eficiÃªncia do sistema:\n","\n","* Testar outros modelos de Embedding - conforme citado, a seleÃ§Ã£o do modelo correto para o sistema RAG Ã© crucial, pois afeta diretamente a precisÃ£o das respostas, alÃ©m da utilizaÃ§Ã£o de recursos e a escalabilidade da aplicaÃ§Ã£o. Escolher modelos que funcionem bem com o idioma e a tarefa em questÃ£o pode melhorar significativamente os resultados. Ao testar diferentes modelos, vocÃª pode identificar qual oferece a melhor combinaÃ§Ã£o de qualidade e eficiÃªncia para as suas necessidades.\n","\n","* Ajustar o prompt fixo (do sistema) - Modificar o prompt do sistema para tornÃ¡-lo mais explÃ­cito sobre as funÃ§Ãµes que a LLM deve desempenhar pode melhorar os resultados. O prompt deve especificar com clareza o que a LLM deve priorizar na resposta e o que deve ser ignorado. Isso orienta o modelo a focar no que Ã© mais relevante para sua aplicaÃ§Ã£o e seu objetivo.\n","\n","* Melhorar o prompt do usuÃ¡rio - lembrar o usuÃ¡rio (colocando um aviso na interface talvez) que quanto mais especÃ­fico for na pergunta maior a chance de aumentar a precisÃ£o das respostas geradas pela LLM. Quanto mais detalhado e claro o pedido, mais relevante serÃ¡ o retorno. Esta prÃ¡tica tambÃ©m ajuda a reduzir ambiguidades que podem prejudicar a interpretaÃ§Ã£o da consulta pelo modelo.\n","\n","* Ajustar o prompt de contextualizaÃ§Ã£o - lembrando que este prompt reformula a pergunta do usuÃ¡rio com base no histÃ³rico da conversa, algo Ãºtil quando a consulta precisa de contexto para ser corretamente interpretada. O prompt de contextualizaÃ§Ã£o (context_q_system_prompt) instrui o modelo a levar o histÃ³rico em consideraÃ§Ã£o; e embora o prompt atual esteja em inglÃªs devido Ã  maior chance de compatibilidade da LLM com este idioma (apesar de ser compatÃ­vel com o nosso), vocÃª pode testÃ¡-lo em portuguÃªs e assim fica fÃ¡cil modificar o texto para maximizar o desempenho no idioma desejado.\n","\n","* Testar outras LLMs - Explorar outros modelos de linguagem, especialmente aqueles que aceitam uma quantidade maior de tokens e tÃªm bom desempenho no idioma escolhido, pode melhorar a performance. Para casos mais exigentes, pode valer a pena considerar soluÃ§Ãµes proprietÃ¡rias como o ChatGPT ou serviÃ§os pagos (como o Groq, citado no Colab 1) que disponibilizam grandes modelos de cÃ³digo aberto. Modelos maiores podem lidar melhor com consultas complexas e fornecer respostas mais elaboradas.\n","\n","* Ajustar os parÃ¢metros de recuperaÃ§Ã£o (k e fetch_k) - Modificar os parÃ¢metros das etapas de recuperaÃ§Ã£o, como os valores de k e fetch_k, pode ter um impacto significativo no desempenho da sua aplicaÃ§Ã£o. Experimente comeÃ§ar com valores menores e aumentÃ¡-los conforme necessÃ¡rio, sempre monitorando o impacto na relevÃ¢ncia e qualidade das respostas. Para mais detalhes, consulte a seÃ§Ã£o da pipeline RAG e o retriever. Outra ideia seria testar outros algoritmos alÃ©m do MMR.\n","\n","* Deixar melhor preparado para aceitar qualquer documento - uma ideia Ã© fazer o preprocessamento de arquivos PDF (ou outros formatos) para adequaÃ§Ã£o ao vector store. Muitas vezes PDFs possuem tabelas ou outras estruturas que dificultam a interpretaÃ§Ã£o; ou ainda, documentos em formatos mais diferentes como HTML, CSV, ou PPTX nÃ£o estÃ£o estruturados para extraÃ§Ã£o ideal de informaÃ§Ãµes. A preparaÃ§Ã£o desses arquivos Ã© crucial para garantir que o conteÃºdo relevante seja corretamente capturado e disponibilizado para o sistema de recuperaÃ§Ã£o.\n"," * Existem soluÃ§Ãµes especializadas automatizam essa transformaÃ§Ã£o, organizando os dados e eliminando informaÃ§Ãµes desnecessÃ¡rias. Isso otimiza o fluxo de trabalho e melhora a precisÃ£o dos resultados. Um exemplo Ã© o serviÃ§o Unstructured (Acesse https://unstructured.io), que facilita a extraÃ§Ã£o de dados complexos de arquivos, tornando-os prontos para uso em bancos de dados vetoriais e frameworks de LLMs, o que aumenta a qualidade da recuperaÃ§Ã£o da informaÃ§Ã£o e o desempenho da aplicaÃ§Ã£o RAG.\n"," * Para usar isso no langchain Ã© simples, vocÃª pode usar o mÃ©todo de Document Loader. Na prÃ¡tica, basta carregar o documento usando o document loader Unstructured (ao invÃ©s do PyPDFLoader que usamos). Mais detalhes aqui: https://python.langchain.com/v0.2/docs/integrations/document_loaders/unstructured_file/\n","\n","\n","Essas estratÃ©gias visam otimizar a eficiÃªncia e a qualidade das respostas do sistema RAG, adaptando-o ao seu caso de uso especÃ­fico.\n","\n","\n"],"metadata":{"id":"Ex1IJrK34UCw"}}]}