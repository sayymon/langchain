{"cells":[{"cell_type":"markdown","metadata":{"id":"fOaSQlfiFLID"},"source":["# Projeto 02 - Chatbot customizado com memória e interface\n","\n","> Nesse projeto você aprenderá como adicionar melhorias ao sistema de chat que criamos, onde será adicionado um sistema para que o bot seja capaz de lembrar  o histórico da conversa e com isso ter maior compreensão do contexto das mensagens, o que é possível através das interações passadas (Chatbots com essa capacidade são conhecidos como \"Context-Aware Chatbot\"). Além disso, veremos como facilmente criar uma interface amigável para sua aplicação usando uma biblioteca versátil chamada Streamlit.\n","\n","Vamos aprender como fazer pelo Colab e também como reaproveitar nosso código para executar em seu ambiente local, o que pode ser mais interessante.\n","\n","Com a ideia de tornar nossa aplicação mais flexível, deixaremos ela preparada para aceitar diferentes modelos e provedores de LLMs, tanto open source (com execução local ou via cloud) e proprietárias (via API).\n","\n","Desse modo, caso esteja usando pelo Colab, essa aplicação vai funcionar inclusive se você selecionar CPU ao invés de GPU.\n","\n"," * Falaremos mais sobre a vantagem de cada método mais tarde, enquanto estivermos desenvolvendo a integração."]},{"cell_type":"markdown","source":["## [ ! ] Como executar em ambiente local\n","\n","* Para executar o código desse projeto em um ambiente local, siga as instruções para instalar as dependências necessárias usando os comandos abaixo. Você pode usar os mesmos comandos de instalação. Para mais detalhes, confira as aulas em vídeo referente à configuração local com o Streamlit.\n","\n","* Você pode executar localmente desde já conforme é mostrado em aula - mas caso esteja com erros de configuração em seu ambiente local, recomendamos fazer pelo Colab antes, para não atrapalhar o fluxo de aprendizagem. Mas caso opte por fazer localmente já também é interessante, pois o Streamlit pede que trabalhemos com arquivo .py e aqui no Colab é .ipynb por causa do Jupyter Notebook, portanto devemos juntar tudo num só arquivo .py  \n","\n","* Além disso, ao executar em ambiente local você pode ir visualizando as alterações mais rapidamente, pois após executar o comando que inicializa o streamlit (exemplo: `!streamlit run projeto2.py`) basta que edite o script .py e salve, e recarregue a página do streamlit, assim verá a mudança. Ou seja, não precisa reexecutar o comando `!streamlit...`)\n","\n","* Antes de rodar seu código localmente, certifique-se de que todas as bibliotecas listadas no comando pip install estejam instaladas.  Caso ainda não as tenha, você pode instalá-las diretamente pelo terminal do VS Code (caso esteja usando essa IDE) ou o terminal/prompt normal.\n"],"metadata":{"id":"GauNtm2ws46r"}},{"cell_type":"markdown","metadata":{"id":"X_CDI-qUEI0m"},"source":["## Instalação e Configuração\n","\n","Precisamos instalar algumas bibliotecas que serão necessárias em nossa aplicação, como o LangChain e sstreamlit (para criação da interface), e mais alguns pacotes necessários e que usamos anteriormente\n","\n","> Se estiver executando localmente: precisa instalar também o pytorch, caso já não tenha instalado (lembrando que no Colab já vem instalado por padrão, basta importar).\n"," * Para evitar problemas de compatibilidade, recomendamos esse comando: `pip install torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu121`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXD-F395R75e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399511316,"user_tz":180,"elapsed":93350,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"c7e4b3be-c93a-4589-c5b6-af13c294ba00"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m934.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m131.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q streamlit langchain sentence-transformers\n","!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"]},{"cell_type":"markdown","source":["> Instalação do Localtunnel\n","\n","Caso esteja executando no Colab, você precisa também instalar o Localtunnel para conseguirmos nos conectar à aplicação gerada com o streamlit.\n","\n","Isso será explicado na etapa em que é feita a inicialização da interface"],"metadata":{"id":"N0qSu4O8tXIh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdYajOzC-1Y6"},"outputs":[],"source":["!npm install localtunnel"]},{"cell_type":"markdown","source":["### Carregando as variáveis de ambiente com o dotenv\n","\n","Utilizaremos a biblioteca dotenv, que simplifica a gestão de variáveis de ambiente ao armazená-las em um arquivo .env."],"metadata":{"id":"zQahRcTktkWu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qWrf4IMAjrz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399547203,"user_tz":180,"elapsed":4853,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"636c2d24-63ec-40e1-a1fc-dfe66f71c45f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n"]}],"source":["!pip install python-dotenv"]},{"cell_type":"markdown","metadata":{"id":"82R6hEnbG98h"},"source":["#### Criação do arquivo .env\n","\n","O comando `%%writefile` permite que a célula do notebook seja salva como um arquivo externo, com o nome especificado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UusxhnSpA4lL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399560287,"user_tz":180,"elapsed":358,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"05cc0f50-1a92-4fc1-b9ad-776470d1a33e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing .env\n"]}],"source":["%%writefile .env\n","HUGGINGFACE_API_KEY==##########\n","HUGGINGFACEHUB_API_TOKEN==##########\n","OPENAI_API_KEY==##########\n","TAVILY_API_KEY=##########\n","SERPAPI_API_KEY=##########\n","LANGCHAIN_API_KEY=##########"]},{"cell_type":"markdown","metadata":{"id":"XO2t7UV3HHiN"},"source":["## Inicialização da interface\n","\n","Agora precisamos definir só algumas configurações do Streamlit e então reunir todo o código em um arquivo .py, desse modo conseguiremos rodar no Colab também   \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gC59Y_AF9Lu-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399609167,"user_tz":180,"elapsed":418,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"fac5ff84-69de-4061-fb7f-fcdd3e2113d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing projeto2.py\n"]}],"source":["%%writefile projeto2.py\n","\n","import streamlit as st\n","from langchain_core.messages import AIMessage, HumanMessage\n","from langchain_core.prompts import MessagesPlaceholder\n","\n","from langchain_ollama import ChatOllama\n","from langchain_openai import ChatOpenAI\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","import torch\n","from langchain_huggingface import ChatHuggingFace\n","from langchain_community.llms import HuggingFaceHub\n","\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","# Configurações do Streamlit\n","st.set_page_config(page_title=\"Seu assistente virtual 🤖\", page_icon=\"🤖\")\n","st.title(\"Seu assistente virtual 🤖\")\n","\n","model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n","\n","def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n","  llm = HuggingFaceHub(\n","      repo_id=model,\n","      model_kwargs={\n","          \"temperature\": temperature,\n","          \"return_full_text\": False,\n","          \"max_new_tokens\": 512,\n","          #\"stop\": [\"<|eot_id|>\"],\n","          # demais parâmetros que desejar\n","      }\n","  )\n","  return llm\n","\n","def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n","    llm = ChatOpenAI(\n","        model=model,\n","        temperature=temperature\n","        # demais parâmetros que desejar\n","    )\n","    return llm\n","\n","def model_ollama(model=\"phi3\", temperature=0.1):\n","    llm = ChatOllama(\n","        model=model,\n","        temperature=temperature,\n","    )\n","    return llm\n","\n","\n","def model_response(user_query, chat_history, model_class):\n","\n","    ## Carregamento da LLM\n","    if model_class == \"hf_hub\":\n","        llm = model_hf_hub()\n","    elif model_class == \"openai\":\n","        llm = model_openai()\n","    elif model_class == \"ollama\":\n","        llm = model_ollama()\n","\n","    ## Definição dos prompts\n","    system_prompt = \"\"\"\n","    Você é um assistente prestativo e está respondendo perguntas gerais. Responda em {language}.\n","    \"\"\"\n","    # corresponde à variável do idioma em nosso template\n","    language = \"português\"\n","\n","    # Adequando à pipeline\n","    if model_class.startswith(\"hf\"):\n","        user_prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n","    else:\n","        user_prompt = \"{input}\"\n","\n","    prompt_template = ChatPromptTemplate.from_messages([\n","        (\"system\", system_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"user\", user_prompt)\n","    ])\n","\n","    ## Criação da Chain\n","    chain = prompt_template | llm | StrOutputParser()\n","\n","    ## Retorno da resposta / Stream\n","    return chain.stream({\n","        \"chat_history\": chat_history,\n","        \"input\": user_query,\n","        \"language\": language\n","    })\n","\n","\n","if \"chat_history\" not in st.session_state:\n","    st.session_state.chat_history = [\n","        AIMessage(content=\"Olá, sou o seu assistente virtual! Como posso ajudar você?\"),\n","    ]\n","\n","for message in st.session_state.chat_history:\n","    if isinstance(message, AIMessage):\n","        with st.chat_message(\"AI\"):\n","            st.write(message.content)\n","    elif isinstance(message, HumanMessage):\n","        with st.chat_message(\"Human\"):\n","            st.write(message.content)\n","\n","user_query = st.chat_input(\"Digite sua mensagem aqui...\")\n","if user_query is not None and user_query != \"\":\n","    st.session_state.chat_history.append(HumanMessage(content=user_query))\n","\n","    with st.chat_message(\"Human\"):\n","        st.markdown(user_query)\n","\n","    with st.chat_message(\"AI\"):\n","        resp = st.write_stream(model_response(user_query, st.session_state.chat_history, model_class))\n","        print(st.session_state.chat_history)\n","\n","    st.session_state.chat_history.append(AIMessage(content=resp))"]},{"cell_type":"markdown","metadata":{"id":"-jrLVCt24tDC"},"source":["### Execução do Streamlit\n","\n","Tendo nosso script pronto, basta executar o comando abaixo para rodar a nossa aplicação pelo streamlit.\n","Isso fará com que a aplicação do Streamlit seja executada em segundo plano."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDzu4rHv9T3f"},"outputs":[],"source":["!streamlit run projeto2.py &>/content/logs.txt &"]},{"cell_type":"markdown","source":["\n","\n","Observação:\n","* O `&` no final permite que o Colab continue executando outras células sem esperar que o aplicativo Streamlit termine.\n","\n","* ao rodar localmente não é necessário o `&>/content/logs.txt &`\n","\n","* aqui usamos pois o Colab não exibe no terminal a informação que precisamos, pois não podemos visualizá-lo pelo Colab (já que ele funciona de outro modo e não temos acesso ao terminal que é atualizado em tempo real - pelo menos na versão gratuita).\n","\n","\n","* O que esse trecho faz portanto é adicionar os logs do comando a um arquivo chamado `logs.txt`\n","\n","\n",">  Caso esteja acessando localmente agora basta acessar o link que irá aparecer no terminal (local URL ou Network URL, caso esteja em outro dispositivo na mesma rede).\n"," *  Para o Colab, precisa de mais um comando para abrir nossa aplicação (veja abaixo)"],"metadata":{"id":"COvIzXDGvpjI"}},{"cell_type":"markdown","metadata":{"id":"4ggfAm5wCDBs"},"source":["### Acesso com LocalTunnel\n","\n","Antes de conectar com o localtunnel, você precisa obter o IP externo, que será usado como a senha ao fazer o launch da aplicação nessa próxima etapa.\n","\n","Tem duas maneiras de fazer isso:\n","\n","1) com o comando abaixo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2HtjvqKGMMr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399777180,"user_tz":180,"elapsed":7,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"0d79fa50-090f-4c12-8d11-75b526fada39"},"outputs":[{"output_type":"stream","name":"stdout","text":["34.125.58.9\n"]}],"source":["!wget -q -O - ipv4.icanhazip.com"]},{"cell_type":"markdown","source":["2) Ou, como alternativa, faça desse modo:\n","\n"," * Abra o painel lateral do Colab\n"," * Clique sobre o arquivo logs.txt. Aqui mostra o que seria exibido no terminal\n"," * Selecione o número IP correspondente ao External URL. Somente o número IP com os pontos, sem o http:// ou a porta\n","  * Por exemplo: `35.184.1.10`"],"metadata":{"id":"EaA_NWZ4vxkE"}},{"cell_type":"markdown","source":["Pronto, agora basta executar o comando abaixo.\n","\n","Esse comando usa npx localtunnel para \"expor\" o aplicativo Streamlit em execução local para a internet. O aplicativo é hospedado na porta 8501, e o localtunnel fornece uma URL pública por meio da qual o aplicativo pode ser acessado.\n","\n","Então, entre no link que aparece na saída e informar o IP no campo Tunnel Password. Logo em seguida, clique no botão e aguarde o interface ser inicializada"],"metadata":{"id":"QSkMYtN4zLnp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EqkzAQE-5rp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399947260,"user_tz":180,"elapsed":145558,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"57f362ad-1e34-41a9-c5da-1707d42899cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["your url is: https://four-eels-enjoy.loca.lt\n","^C\n"]}],"source":["!npx localtunnel --port 8501"]},{"cell_type":"markdown","source":["Observação:\n"," * Se der algum erro, recarregue a página e aguarde mais alguns instantes.\n"," * Caso esteja usando um método que não seja por API então é normal que na primeira execução leve um pouco mais de tempo.\n","  * Se velocidade for um fator muito determinante, recomendamos usar soluções onde o processamento é feito em um servidor externo e conecta-se via API como HF, Open AI ou Groq"],"metadata":{"id":"vaj0JGZ4w0I-"}},{"cell_type":"markdown","metadata":{"id":"ojxqLY40kZ_Q"},"source":["---\n","\n","## Criando seu próprio prompt\n","> **Dica de estrutura para criar seu próprio Prompt**\n","\n","Você pode modificar à vontade o prompt para que atenda ao seu objetivo. Você pode usar esse formato:\n","\n","* Introdução: Comece com uma breve introdução ao tema, definindo o conceito básico.\n","* Explicação: Forneça uma explicação detalhada, mas simples, sobre o conceito. Utilize exemplos práticos ou analogias quando necessário para facilitar a compreensão.\n","* Passos ou Componentes: Se o conceito tiver vários componentes ou etapas, liste e explique cada um de forma concisa.\n","* Aplicações: Dê exemplos de como esse conceito é aplicado na prática ou em contextos reais.\n","* Resumo: Conclua com um resumo das principais ideias apresentadas.\n","* Orientações Adicionais: Caso seja relevante, ofereça dicas ou orientações adicionais para aprofundamento no tema.\n","\n","Palavras-chave relevantes para adicionar ao seu prompt e informar como deseja que seja sua resposta:\n","* Claro, Objetivo, Simples, Exemplo prático, Analogia, Explicação detalhada, Resumo\n","\n","Outras ideias:\n","* explique [x] para alguém leigo; explique de modo fácil como se tivesse explicando para uma criança.\n","\n","Indo além:\n","* você pode também procurar frameworks de prompt para fazer com que LLM desempenhe da melhor forma o papel desejado. Por exemplo, o framwork [COSTAR](https://medium.com/@frugalzentennial/unlocking-the-power-of-costar-prompt-engineering-a-guide-and-example-on-converting-goals-into-dc5751ce9875), método que garante que todos os aspectos-chave que influenciam a resposta de um LLM sejam considerados, resultando em respostas de saída mais personalizadas.\n","* Quando o objetivo é fazer com que o modelo desempenhe um papel específico ou atue de um determinado modo, é chamado de role-playing, e tem crescido muito as pesquisas em cima disso (como por exemplo [esse paper](https://arxiv.org/abs/2406.00627)).\n"]},{"cell_type":"markdown","source":["## Alternativa ao Streamlit\n","\n","Criando nossa própria aplicação com o streamlit nos garante uma certa liberdade, principalmente porque ao criar \"do zero\" podemos deixar do jeito que queremos. Mas existem outras formas mais prontas e com a interface já criada e disponível para uso, não exigindo lidar com código. Como nossa intenção aqui é também trabalhar com o código fonte e não depender unicamente de um programa/interface então não acabamos abordando, mas caso tenha interesse de usar uma alternativa assim então temos algumas recomendações:\n","\n","* Open WebUI - https://github.com/open-webui/open-webui\n","* GPT4All - https://gpt4all.io/index.html\n","* AnythingLLM - https://anythingllm.com\n","\n","Essas soluções possuem várias outras funcionalidades interessantes e integrações, portanto pode ser uma boa ideia checar caso tenha interesse em explorar mais LLMs."],"metadata":{"id":"DL3ie4MprOha"}}],"metadata":{"colab":{"provenance":[{"file_id":"1Y-vZbs_sEa2Y5wq0jbqSoQ60w5OJEAv2","timestamp":1725397980979}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}