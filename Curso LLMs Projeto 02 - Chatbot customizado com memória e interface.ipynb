{"cells":[{"cell_type":"markdown","metadata":{"id":"fOaSQlfiFLID"},"source":["# Projeto 02 - Chatbot customizado com mem√≥ria e interface\n","\n","> Nesse projeto voc√™ aprender√° como adicionar melhorias ao sistema de chat que criamos, onde ser√° adicionado um sistema para que o bot seja capaz de lembrar  o hist√≥rico da conversa e com isso ter maior compreens√£o do contexto das mensagens, o que √© poss√≠vel atrav√©s das intera√ß√µes passadas (Chatbots com essa capacidade s√£o conhecidos como \"Context-Aware Chatbot\"). Al√©m disso, veremos como facilmente criar uma interface amig√°vel para sua aplica√ß√£o usando uma biblioteca vers√°til chamada Streamlit.\n","\n","Vamos aprender como fazer pelo Colab e tamb√©m como reaproveitar nosso c√≥digo para executar em seu ambiente local, o que pode ser mais interessante.\n","\n","Com a ideia de tornar nossa aplica√ß√£o mais flex√≠vel, deixaremos ela preparada para aceitar diferentes modelos e provedores de LLMs, tanto open source (com execu√ß√£o local ou via cloud) e propriet√°rias (via API).\n","\n","Desse modo, caso esteja usando pelo Colab, essa aplica√ß√£o vai funcionar inclusive se voc√™ selecionar CPU ao inv√©s de GPU.\n","\n"," * Falaremos mais sobre a vantagem de cada m√©todo mais tarde, enquanto estivermos desenvolvendo a integra√ß√£o."]},{"cell_type":"markdown","source":["## [ ! ] Como executar em ambiente local\n","\n","* Para executar o c√≥digo desse projeto em um ambiente local, siga as instru√ß√µes para instalar as depend√™ncias necess√°rias usando os comandos abaixo. Voc√™ pode usar os mesmos comandos de instala√ß√£o. Para mais detalhes, confira as aulas em v√≠deo referente √† configura√ß√£o local com o Streamlit.\n","\n","* Voc√™ pode executar localmente desde j√° conforme √© mostrado em aula - mas caso esteja com erros de configura√ß√£o em seu ambiente local, recomendamos fazer pelo Colab antes, para n√£o atrapalhar o fluxo de aprendizagem. Mas caso opte por fazer localmente j√° tamb√©m √© interessante, pois o Streamlit pede que trabalhemos com arquivo .py e aqui no Colab √© .ipynb por causa do Jupyter Notebook, portanto devemos juntar tudo num s√≥ arquivo .py  \n","\n","* Al√©m disso, ao executar em ambiente local voc√™ pode ir visualizando as altera√ß√µes mais rapidamente, pois ap√≥s executar o comando que inicializa o streamlit (exemplo: `!streamlit run projeto2.py`) basta que edite o script .py e salve, e recarregue a p√°gina do streamlit, assim ver√° a mudan√ßa. Ou seja, n√£o precisa reexecutar o comando `!streamlit...`)\n","\n","* Antes de rodar seu c√≥digo localmente, certifique-se de que todas as bibliotecas listadas no comando pip install estejam instaladas.  Caso ainda n√£o as tenha, voc√™ pode instal√°-las diretamente pelo terminal do VS Code (caso esteja usando essa IDE) ou o terminal/prompt normal.\n"],"metadata":{"id":"GauNtm2ws46r"}},{"cell_type":"markdown","metadata":{"id":"X_CDI-qUEI0m"},"source":["## Instala√ß√£o e Configura√ß√£o\n","\n","Precisamos instalar algumas bibliotecas que ser√£o necess√°rias em nossa aplica√ß√£o, como o LangChain e sstreamlit (para cria√ß√£o da interface), e mais alguns pacotes necess√°rios e que usamos anteriormente\n","\n","> Se estiver executando localmente: precisa instalar tamb√©m o pytorch, caso j√° n√£o tenha instalado (lembrando que no Colab j√° vem instalado por padr√£o, basta importar).\n"," * Para evitar problemas de compatibilidade, recomendamos esse comando: `pip install torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cu121`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXD-F395R75e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399511316,"user_tz":180,"elapsed":93350,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"c7e4b3be-c93a-4589-c5b6-af13c294ba00"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m934.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m131.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q streamlit langchain sentence-transformers\n","!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"]},{"cell_type":"markdown","source":["> Instala√ß√£o do Localtunnel\n","\n","Caso esteja executando no Colab, voc√™ precisa tamb√©m instalar o Localtunnel para conseguirmos nos conectar √† aplica√ß√£o gerada com o streamlit.\n","\n","Isso ser√° explicado na etapa em que √© feita a inicializa√ß√£o da interface"],"metadata":{"id":"N0qSu4O8tXIh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdYajOzC-1Y6"},"outputs":[],"source":["!npm install localtunnel"]},{"cell_type":"markdown","source":["### Carregando as vari√°veis de ambiente com o dotenv\n","\n","Utilizaremos a biblioteca dotenv, que simplifica a gest√£o de vari√°veis de ambiente ao armazen√°-las em um arquivo .env."],"metadata":{"id":"zQahRcTktkWu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qWrf4IMAjrz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399547203,"user_tz":180,"elapsed":4853,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"636c2d24-63ec-40e1-a1fc-dfe66f71c45f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n"]}],"source":["!pip install python-dotenv"]},{"cell_type":"markdown","metadata":{"id":"82R6hEnbG98h"},"source":["#### Cria√ß√£o do arquivo .env\n","\n","O comando `%%writefile` permite que a c√©lula do notebook seja salva como um arquivo externo, com o nome especificado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UusxhnSpA4lL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399560287,"user_tz":180,"elapsed":358,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"05cc0f50-1a92-4fc1-b9ad-776470d1a33e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing .env\n"]}],"source":["%%writefile .env\n","HUGGINGFACE_API_KEY==##########\n","HUGGINGFACEHUB_API_TOKEN==##########\n","OPENAI_API_KEY==##########\n","TAVILY_API_KEY=##########\n","SERPAPI_API_KEY=##########\n","LANGCHAIN_API_KEY=##########"]},{"cell_type":"markdown","metadata":{"id":"XO2t7UV3HHiN"},"source":["## Inicializa√ß√£o da interface\n","\n","Agora precisamos definir s√≥ algumas configura√ß√µes do Streamlit e ent√£o reunir todo o c√≥digo em um arquivo .py, desse modo conseguiremos rodar no Colab tamb√©m   \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gC59Y_AF9Lu-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399609167,"user_tz":180,"elapsed":418,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"fac5ff84-69de-4061-fb7f-fcdd3e2113d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing projeto2.py\n"]}],"source":["%%writefile projeto2.py\n","\n","import streamlit as st\n","from langchain_core.messages import AIMessage, HumanMessage\n","from langchain_core.prompts import MessagesPlaceholder\n","\n","from langchain_ollama import ChatOllama\n","from langchain_openai import ChatOpenAI\n","\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","import torch\n","from langchain_huggingface import ChatHuggingFace\n","from langchain_community.llms import HuggingFaceHub\n","\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","# Configura√ß√µes do Streamlit\n","st.set_page_config(page_title=\"Seu assistente virtual ü§ñ\", page_icon=\"ü§ñ\")\n","st.title(\"Seu assistente virtual ü§ñ\")\n","\n","model_class = \"hf_hub\" # @param [\"hf_hub\", \"openai\", \"ollama\"]\n","\n","def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n","  llm = HuggingFaceHub(\n","      repo_id=model,\n","      model_kwargs={\n","          \"temperature\": temperature,\n","          \"return_full_text\": False,\n","          \"max_new_tokens\": 512,\n","          #\"stop\": [\"<|eot_id|>\"],\n","          # demais par√¢metros que desejar\n","      }\n","  )\n","  return llm\n","\n","def model_openai(model=\"gpt-4o-mini\", temperature=0.1):\n","    llm = ChatOpenAI(\n","        model=model,\n","        temperature=temperature\n","        # demais par√¢metros que desejar\n","    )\n","    return llm\n","\n","def model_ollama(model=\"phi3\", temperature=0.1):\n","    llm = ChatOllama(\n","        model=model,\n","        temperature=temperature,\n","    )\n","    return llm\n","\n","\n","def model_response(user_query, chat_history, model_class):\n","\n","    ## Carregamento da LLM\n","    if model_class == \"hf_hub\":\n","        llm = model_hf_hub()\n","    elif model_class == \"openai\":\n","        llm = model_openai()\n","    elif model_class == \"ollama\":\n","        llm = model_ollama()\n","\n","    ## Defini√ß√£o dos prompts\n","    system_prompt = \"\"\"\n","    Voc√™ √© um assistente prestativo e est√° respondendo perguntas gerais. Responda em {language}.\n","    \"\"\"\n","    # corresponde √† vari√°vel do idioma em nosso template\n","    language = \"portugu√™s\"\n","\n","    # Adequando √† pipeline\n","    if model_class.startswith(\"hf\"):\n","        user_prompt = \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n","    else:\n","        user_prompt = \"{input}\"\n","\n","    prompt_template = ChatPromptTemplate.from_messages([\n","        (\"system\", system_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"user\", user_prompt)\n","    ])\n","\n","    ## Cria√ß√£o da Chain\n","    chain = prompt_template | llm | StrOutputParser()\n","\n","    ## Retorno da resposta / Stream\n","    return chain.stream({\n","        \"chat_history\": chat_history,\n","        \"input\": user_query,\n","        \"language\": language\n","    })\n","\n","\n","if \"chat_history\" not in st.session_state:\n","    st.session_state.chat_history = [\n","        AIMessage(content=\"Ol√°, sou o seu assistente virtual! Como posso ajudar voc√™?\"),\n","    ]\n","\n","for message in st.session_state.chat_history:\n","    if isinstance(message, AIMessage):\n","        with st.chat_message(\"AI\"):\n","            st.write(message.content)\n","    elif isinstance(message, HumanMessage):\n","        with st.chat_message(\"Human\"):\n","            st.write(message.content)\n","\n","user_query = st.chat_input(\"Digite sua mensagem aqui...\")\n","if user_query is not None and user_query != \"\":\n","    st.session_state.chat_history.append(HumanMessage(content=user_query))\n","\n","    with st.chat_message(\"Human\"):\n","        st.markdown(user_query)\n","\n","    with st.chat_message(\"AI\"):\n","        resp = st.write_stream(model_response(user_query, st.session_state.chat_history, model_class))\n","        print(st.session_state.chat_history)\n","\n","    st.session_state.chat_history.append(AIMessage(content=resp))"]},{"cell_type":"markdown","metadata":{"id":"-jrLVCt24tDC"},"source":["### Execu√ß√£o do Streamlit\n","\n","Tendo nosso script pronto, basta executar o comando abaixo para rodar a nossa aplica√ß√£o pelo streamlit.\n","Isso far√° com que a aplica√ß√£o do Streamlit seja executada em segundo plano."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDzu4rHv9T3f"},"outputs":[],"source":["!streamlit run projeto2.py &>/content/logs.txt &"]},{"cell_type":"markdown","source":["\n","\n","Observa√ß√£o:\n","* O `&` no final permite que o Colab continue executando outras c√©lulas sem esperar que o aplicativo Streamlit termine.\n","\n","* ao rodar localmente n√£o √© necess√°rio o `&>/content/logs.txt &`\n","\n","* aqui usamos pois o Colab n√£o exibe no terminal a informa√ß√£o que precisamos, pois n√£o podemos visualiz√°-lo pelo Colab (j√° que ele funciona de outro modo e n√£o temos acesso ao terminal que √© atualizado em tempo real - pelo menos na vers√£o gratuita).\n","\n","\n","* O que esse trecho faz portanto √© adicionar os logs do comando a um arquivo chamado `logs.txt`\n","\n","\n",">  Caso esteja acessando localmente agora basta acessar o link que ir√° aparecer no terminal (local URL ou Network URL, caso esteja em outro dispositivo na mesma rede).\n"," *  Para o Colab, precisa de mais um comando para abrir nossa aplica√ß√£o (veja abaixo)"],"metadata":{"id":"COvIzXDGvpjI"}},{"cell_type":"markdown","metadata":{"id":"4ggfAm5wCDBs"},"source":["### Acesso com LocalTunnel\n","\n","Antes de conectar com o localtunnel, voc√™ precisa obter o IP externo, que ser√° usado como a senha ao fazer o launch da aplica√ß√£o nessa pr√≥xima etapa.\n","\n","Tem duas maneiras de fazer isso:\n","\n","1) com o comando abaixo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2HtjvqKGMMr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399777180,"user_tz":180,"elapsed":7,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"0d79fa50-090f-4c12-8d11-75b526fada39"},"outputs":[{"output_type":"stream","name":"stdout","text":["34.125.58.9\n"]}],"source":["!wget -q -O - ipv4.icanhazip.com"]},{"cell_type":"markdown","source":["2) Ou, como alternativa, fa√ßa desse modo:\n","\n"," * Abra o painel lateral do Colab\n"," * Clique sobre o arquivo logs.txt. Aqui mostra o que seria exibido no terminal\n"," * Selecione o n√∫mero IP correspondente ao External URL. Somente o n√∫mero IP com os pontos, sem o http:// ou a porta\n","  * Por exemplo: `35.184.1.10`"],"metadata":{"id":"EaA_NWZ4vxkE"}},{"cell_type":"markdown","source":["Pronto, agora basta executar o comando abaixo.\n","\n","Esse comando usa npx localtunnel para \"expor\" o aplicativo Streamlit em execu√ß√£o local para a internet. O aplicativo √© hospedado na porta 8501, e o localtunnel fornece uma URL p√∫blica por meio da qual o aplicativo pode ser acessado.\n","\n","Ent√£o, entre no link que aparece na sa√≠da e informar o IP no campo Tunnel Password. Logo em seguida, clique no bot√£o e aguarde o interface ser inicializada"],"metadata":{"id":"QSkMYtN4zLnp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EqkzAQE-5rp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725399947260,"user_tz":180,"elapsed":145558,"user":{"displayName":"Jones Granatyr","userId":"10042675233362078631"}},"outputId":"57f362ad-1e34-41a9-c5da-1707d42899cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["your url is: https://four-eels-enjoy.loca.lt\n","^C\n"]}],"source":["!npx localtunnel --port 8501"]},{"cell_type":"markdown","source":["Observa√ß√£o:\n"," * Se der algum erro, recarregue a p√°gina e aguarde mais alguns instantes.\n"," * Caso esteja usando um m√©todo que n√£o seja por API ent√£o √© normal que na primeira execu√ß√£o leve um pouco mais de tempo.\n","  * Se velocidade for um fator muito determinante, recomendamos usar solu√ß√µes onde o processamento √© feito em um servidor externo e conecta-se via API como HF, Open AI ou Groq"],"metadata":{"id":"vaj0JGZ4w0I-"}},{"cell_type":"markdown","metadata":{"id":"ojxqLY40kZ_Q"},"source":["---\n","\n","## Criando seu pr√≥prio prompt\n","> **Dica de estrutura para criar seu pr√≥prio Prompt**\n","\n","Voc√™ pode modificar √† vontade o prompt para que atenda ao seu objetivo. Voc√™ pode usar esse formato:\n","\n","* Introdu√ß√£o: Comece com uma breve introdu√ß√£o ao tema, definindo o conceito b√°sico.\n","* Explica√ß√£o: Forne√ßa uma explica√ß√£o detalhada, mas simples, sobre o conceito. Utilize exemplos pr√°ticos ou analogias quando necess√°rio para facilitar a compreens√£o.\n","* Passos ou Componentes: Se o conceito tiver v√°rios componentes ou etapas, liste e explique cada um de forma concisa.\n","* Aplica√ß√µes: D√™ exemplos de como esse conceito √© aplicado na pr√°tica ou em contextos reais.\n","* Resumo: Conclua com um resumo das principais ideias apresentadas.\n","* Orienta√ß√µes Adicionais: Caso seja relevante, ofere√ßa dicas ou orienta√ß√µes adicionais para aprofundamento no tema.\n","\n","Palavras-chave relevantes para adicionar ao seu prompt e informar como deseja que seja sua resposta:\n","* Claro, Objetivo, Simples, Exemplo pr√°tico, Analogia, Explica√ß√£o detalhada, Resumo\n","\n","Outras ideias:\n","* explique [x] para algu√©m leigo; explique de modo f√°cil como se tivesse explicando para uma crian√ßa.\n","\n","Indo al√©m:\n","* voc√™ pode tamb√©m procurar frameworks de prompt para fazer com que LLM desempenhe da melhor forma o papel desejado. Por exemplo, o framwork [COSTAR](https://medium.com/@frugalzentennial/unlocking-the-power-of-costar-prompt-engineering-a-guide-and-example-on-converting-goals-into-dc5751ce9875), m√©todo que garante que todos os aspectos-chave que influenciam a resposta de um LLM sejam considerados, resultando em respostas de sa√≠da mais personalizadas.\n","* Quando o objetivo √© fazer com que o modelo desempenhe um papel espec√≠fico ou atue de um determinado modo, √© chamado de role-playing, e tem crescido muito as pesquisas em cima disso (como por exemplo [esse paper](https://arxiv.org/abs/2406.00627)).\n"]},{"cell_type":"markdown","source":["## Alternativa ao Streamlit\n","\n","Criando nossa pr√≥pria aplica√ß√£o com o streamlit nos garante uma certa liberdade, principalmente porque ao criar \"do zero\" podemos deixar do jeito que queremos. Mas existem outras formas mais prontas e com a interface j√° criada e dispon√≠vel para uso, n√£o exigindo lidar com c√≥digo. Como nossa inten√ß√£o aqui √© tamb√©m trabalhar com o c√≥digo fonte e n√£o depender unicamente de um programa/interface ent√£o n√£o acabamos abordando, mas caso tenha interesse de usar uma alternativa assim ent√£o temos algumas recomenda√ß√µes:\n","\n","* Open WebUI - https://github.com/open-webui/open-webui\n","* GPT4All - https://gpt4all.io/index.html\n","* AnythingLLM - https://anythingllm.com\n","\n","Essas solu√ß√µes possuem v√°rias outras funcionalidades interessantes e integra√ß√µes, portanto pode ser uma boa ideia checar caso tenha interesse em explorar mais LLMs."],"metadata":{"id":"DL3ie4MprOha"}}],"metadata":{"colab":{"provenance":[{"file_id":"1Y-vZbs_sEa2Y5wq0jbqSoQ60w5OJEAv2","timestamp":1725397980979}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}